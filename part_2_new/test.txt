test 1 (vergine)    ----->  Slot F1:  0.9189851268591427
                            Intent Accuracy: 0.9372900335946248

test 2 (dropout)    ----->  Slot F1:  0.7164548022598871
                            Intent Accuracy: 0.6293393057110862

test 3 (dropout + bidir)    ----->  Slot F1:  0.823693866853049
                                    Intent Accuracy: 0.7693169092945129



Slot F1:  0
Intent Accuracy: 0.05375139977603583


(???)





La intent calssification si concentra sulla previsione dell'intento della query, mentre lo slot filling estrae concetti semantici

La mancanza di dati etichettati dall'uomo per l'NLU e altre attività di elaborazione del linguaggio naturale (NLP) porta a una scarsa capacità di generalizzazione. 
I modelli pre-addestrati possono essere sottoposti a fine-tuning su compiti NLP. Più recentemente, è stata proposta una tecnica di pre-addestramento, 
Bidirectional Encoder Representations from Transformers (BERT), che ha creato modelli all'avanguardia per una vasta gamma di compiti NLP, tra cui il 
question answering (SQuAD v1.1), l'inferenza del linguaggio naturale e altri.





